{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73115924-8033-4ca0-a62a-80d01afb7b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo de neurona simple con dos algoritmos de aprendizaje (SGD y Adam)\n",
    "\n",
    "# Solo usasmosNumPy porque implementamos todo desde cero: pesos, gradientes, actualizaciones, etc.\n",
    "import numpy as np\n",
    "\n",
    "# ============================\n",
    "# Funciones de activaci√≥n\n",
    "# ============================\n",
    "\n",
    "# ReLU deja pasar valores positivos y convierte los negativos en 0.\n",
    "\n",
    "# Su derivada es 1 cuando ùë•>0 y 0 cuando ùë•‚â§0.\n",
    "# Se usa para introducir no linealidad en la neurona.\n",
    "# Estas funciones se pasan como par√°metros al modelo.\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_deriv(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_deriv(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "# ============================\n",
    "# Clase NeuralNetwork (neurona simple)\n",
    "# ============================\n",
    "\n",
    "# Esta es la parte central del Notebook. Define c√≥mo funciona la neurona y c√≥mo aprende.\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, n_inputs, n_outputs, activation, activation_deriv):\n",
    "        # w: matriz de pesos inicializada aleatoriamente.\n",
    "        self.w = np.random.randn(n_inputs, n_outputs) * 0.1\n",
    "        # b: vector de sesgos inicializado en cero.\n",
    "        self.b = np.zeros((1, n_outputs))\n",
    "        self.activation = activation\n",
    "        self.activation_deriv = activation_deriv\n",
    "\n",
    "        # Par√°metros Adam, inicializaci√≥n\n",
    "        # Estos almacenan los momentos del gradiente y el contador de pasos.\n",
    "        self.m_w = np.zeros_like(self.w)\n",
    "        self.v_w = np.zeros_like(self.w)\n",
    "        self.m_b = np.zeros_like(self.b)\n",
    "        self.v_b = np.zeros_like(self.b)\n",
    "        self.t = 0\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Calcula la salida lineal ùëß=ùëãùëä+ùëè.\n",
    "        # Aplica la funci√≥n de activaci√≥n (ReLU por defecto).\n",
    "        # Guarda z y a para el c√°lculo del gradiente.\n",
    "        self.z = X @ self.w + self.b\n",
    "        self.a = self.activation(self.z)\n",
    "        return self.a\n",
    "\n",
    "    # C√°lculo de gradientes:\n",
    "    # Aqu√≠ se implementa MSE (mean squared error) como funci√≥n de p√©rdida.\n",
    "    # error: diferencia entre predicci√≥n y etiqueta.\n",
    "    # dz: gradiente despu√©s de la activaci√≥n.\n",
    "    # dw, db: gradientes de pesos y sesgos.\n",
    "    # loss: p√©rdida promedio.\n",
    "    def compute_grads(self, X, y):\n",
    "        # \n",
    "        m = X.shape[0]\n",
    "        error = self.a - y\n",
    "        dz = error * self.activation_deriv(self.z)\n",
    "        dw = (X.T @ dz) / m\n",
    "        db = np.sum(dz, axis=0, keepdims=True) / m\n",
    "        loss = np.mean(error**2)\n",
    "        return dw, db, loss\n",
    "\n",
    "    # ============================\n",
    "    # SGD\n",
    "    # ============================\n",
    "    # Entrenamiento con SGD\n",
    "    # Actualiza pesos usando gradiente descendente cl√°sico.\n",
    "    # Puede usar mini-batches si batch_size > 1.\n",
    "    # Mezcla los datos en cada √©poca si shuffle=True.\n",
    "    # Este m√©todo es simple pero puede ser inestable.\n",
    "    def train_sgd(self, X, y, epochs=200, lr=0.01):\n",
    "        losses = []\n",
    "        for _ in range(epochs):\n",
    "            self.forward(X)\n",
    "            dw, db, loss = self.compute_grads(X, y)\n",
    "            self.w -= lr * dw\n",
    "            self.b -= lr * db\n",
    "            losses.append(loss)\n",
    "        return losses\n",
    "\n",
    "    # ============================\n",
    "    # Adam\n",
    "    # ============================\n",
    "    # Adam es un optimizador m√°s avanzado que ajusta autom√°ticamente la tasa de aprendizaje.\n",
    "\n",
    "    def train_adam(self, X, y, epochs=200, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8):\n",
    "        losses = []\n",
    "        for _ in range(epochs):\n",
    "            self.forward(X)\n",
    "            dw, db, loss = self.compute_grads(X, y)\n",
    "\n",
    "            self.t += 1\n",
    "\n",
    "            # Momentos\n",
    "            # m_w: promedio m√≥vil del gradiente (primer momento).\n",
    "            # v_w: promedio m√≥vil del cuadrado del gradiente (segundo momento).\n",
    "            self.m_w = beta1 * self.m_w + (1 - beta1) * dw\n",
    "            self.v_w = beta2 * self.v_w + (1 - beta2) * (dw**2)\n",
    "\n",
    "            self.m_b = beta1 * self.m_b + (1 - beta1) * db\n",
    "            self.v_b = beta2 * self.v_b + (1 - beta2) * (db**2)\n",
    "\n",
    "            # Correcci√≥n de sesgo\n",
    "            m_w_hat = self.m_w / (1 - beta1**self.t)\n",
    "            v_w_hat = self.v_w / (1 - beta2**self.t)\n",
    "\n",
    "            m_b_hat = self.m_b / (1 - beta1**self.t)\n",
    "            v_b_hat = self.v_b / (1 - beta2**self.t)\n",
    "\n",
    "            # Actualizaci√≥n final\n",
    "            # Adam ajusta cada peso de forma independiente seg√∫n su historial de gradientes.\n",
    "            self.w -= lr * m_w_hat / (np.sqrt(v_w_hat) + eps)\n",
    "            self.b -= lr * m_b_hat / (np.sqrt(v_b_hat) + eps)\n",
    "\n",
    "            losses.append(loss)\n",
    "        return losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "976d3523-89e8-439e-b552-3505539a89b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Clasificaci√≥n del dataset Iris usando SGD y Adam\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Cargar datos\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target.reshape(-1, 1)\n",
    "\n",
    "# One-hot encoding\n",
    "enc = OneHotEncoder(sparse_output=False)\n",
    "y_onehot = enc.fit_transform(y)\n",
    "\n",
    "# Escalado\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Divisi√≥n\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_onehot, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Entrenar modelos\n",
    "sgd_model = NeuralNetwork(4, 3, relu, relu_deriv)\n",
    "loss_sgd = sgd_model.train_sgd(X_train, y_train, epochs=300, lr=0.01)\n",
    "\n",
    "adam_model = NeuralNetwork(4, 3, relu, relu_deriv)\n",
    "loss_adam = adam_model.train_adam(X_train, y_train, epochs=300, lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa6ee4af-3a88-4a7a-a09c-593892e246bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n de precisi√≥n\n",
    "\n",
    "def predict_classes(model, X):\n",
    "    y_pred = model.forward(X)\n",
    "    return np.argmax(y_pred, axis=1)\n",
    "\n",
    "def accuracy(model, X, y_true):\n",
    "    y_pred = predict_classes(model, X)\n",
    "    y_true = np.argmax(y_true, axis=1)\n",
    "    return np.mean(y_pred == y_true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae8ffb14-8aaa-4a6d-bacf-6a8f0e0a5d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisi√≥n SGD: 0.8888888888888888\n",
      "Precisi√≥n Adam: 0.8666666666666667\n"
     ]
    }
   ],
   "source": [
    "# Precisi√≥n de cada modelo\n",
    "\n",
    "acc_sgd = accuracy(sgd_model, X_test, y_test)\n",
    "acc_adam = accuracy(adam_model, X_test, y_test)\n",
    "\n",
    "print(\"Precisi√≥n SGD:\", acc_sgd)\n",
    "print(\"Precisi√≥n Adam:\", acc_adam)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-2025.12-py312",
   "language": "python",
   "name": "conda-env-anaconda-2025.12-py312-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
